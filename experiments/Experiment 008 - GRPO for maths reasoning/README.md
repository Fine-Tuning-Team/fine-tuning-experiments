# Fine tuning with GRPO for maths reasoning

## Details

- Runtime: Run on kaggle with T4
- Model selected: [Qwen/Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)
- Dataset: [Reasoning dataset](https://huggingface.co/datasets/openai/gsm8k)
- About the dataset: GSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.
- Method: LoRA with GRPO
- Process: Follow the steps in the NB
