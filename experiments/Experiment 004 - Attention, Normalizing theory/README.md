# Some notes and theories on attention and layer normalization

## Details

- Runtime: Run locally

> **NOTE**: These are made for understanding the concepts of attention and layer normalization. This knowledge will be useful when dealing with fine tuning process and when you need to explain what is happening and why it is happening.
