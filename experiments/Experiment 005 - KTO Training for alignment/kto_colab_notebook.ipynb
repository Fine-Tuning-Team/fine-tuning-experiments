{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fju0_fZ9M5Is"
      },
      "source": [
        "# Package Installation and Imports\n",
        "Install required packages including unsloth and flash-attention, and import necessary libraries for the KTO finetuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SESrOQj1M5Is"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "# Install Flash Attention 2 for softcapping support\n",
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\"\n",
        "\n",
        "# Import necessary libraries\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "from typing import List, Literal, Optional\n",
        "from datasets import load_dataset\n",
        "from trl import KTOConfig, KTOTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWsQtPsoM5It"
      },
      "source": [
        "# Model Loading and Configuration\n",
        "Load the pre-trained model and tokenizer using FastLanguageModel, and configure basic parameters like sequence length and quantization settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKeX_vuYM5It",
        "outputId": "eceaed54-f97a-42aa-e454-f121fc5f0122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.1: Fast Qwen2 patching. Transformers:4.46.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# Model Loading and Configuration\n",
        "\n",
        "# Set basic parameters\n",
        "max_seq_length = 4096  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",  # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    # token=\"hf_...\",  # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "# Add proper chat template if missing\n",
        "if tokenizer.chat_template is None:\n",
        "    DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfOvZ0XXM5It"
      },
      "source": [
        "# Dataset Preparation and Processing\n",
        "Load and prepare the KTO dataset, including applying chat templates and processing the data for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRAwRajtM5It"
      },
      "outputs": [],
      "source": [
        "# Dataset Preparation and Processing\n",
        "\n",
        "\n",
        "# Function to apply chat template\n",
        "def apply_chat_template(\n",
        "    example, tokenizer, task: Literal[\"sft\", \"generation\", \"rm\", \"kto\"] = \"sft\", assistant_prefix=\"<|assistant|>\\n\"\n",
        "):\n",
        "    def _strip_prefix(s, pattern):\n",
        "        # Use re.escape to escape any special characters in the pattern\n",
        "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
        "\n",
        "    if task in [\"sft\", \"generation\"]:\n",
        "        messages = example[\"messages\"]\n",
        "        # We add an empty system message if there is none\n",
        "        if messages[0][\"role\"] != \"system\":\n",
        "            messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "        example[\"text\"] = tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True if task == \"generation\" else False\n",
        "        )\n",
        "    elif task == \"rm\":\n",
        "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
        "            chosen_messages = example[\"chosen\"]\n",
        "            rejected_messages = example[\"rejected\"]\n",
        "            # We add an empty system message if there is none\n",
        "            if chosen_messages[0][\"role\"] != \"system\":\n",
        "                chosen_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "            if rejected_messages[0][\"role\"] != \"system\":\n",
        "                rejected_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Could not format example as dialogue for `rm` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
        "            )\n",
        "    elif task == \"dpo\":\n",
        "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
        "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
        "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
        "            # Insert system message\n",
        "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
        "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "            else:\n",
        "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
        "            # TODO: handle case where chosen/rejected also have system messages\n",
        "            chosen_messages = example[\"chosen\"][1:]\n",
        "            rejected_messages = example[\"rejected\"][1:]\n",
        "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
        "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
        "            )\n",
        "            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n",
        "            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
        "            )\n",
        "    elif task == \"kto\":\n",
        "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
        "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
        "            chosen_messages = prompt_messages + [msg for msg in example[\"chosen\"] if msg[\"role\"] == \"assistant\"]\n",
        "            rejected_messages = prompt_messages + [msg for msg in example[\"rejected\"] if msg[\"role\"] == \"assistant\"]\n",
        "            if \"system\" in example:\n",
        "                chosen_messages.insert(0, {\"role\": \"system\", \"content\": example[\"system\"]})\n",
        "                rejected_messages.insert(0, {\"role\": \"system\", \"content\": example[\"system\"]})\n",
        "            example[\"text_chosen\"] = _strip_prefix(tokenizer.apply_chat_template(chosen_messages, tokenize=False), assistant_prefix)\n",
        "            example[\"text_rejected\"] = _strip_prefix(tokenizer.apply_chat_template(rejected_messages, tokenize=False), assistant_prefix)\n",
        "        else:\n",
        "            raise ValueError(f\"Could not format example as dialogue for `kto` task!\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Task {task} not supported, please ensure that the provided task is one of {['sft', 'generation', 'rm', 'dpo', 'kto']}\"\n",
        "        )\n",
        "    return example\n",
        "\n",
        "# Load the KTO dataset\n",
        "raw_datasets = load_dataset(\"trl-lib/kto-mix-14k\")\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "\n",
        "# Take a subset of the training data, I only use 1000 examples here\n",
        "train_subset = train_dataset.select(range(1000))  # Use first 1000 examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcAEPbyjM5It"
      },
      "source": [
        "# Model Training Setup\n",
        "Configure the LoRA adapters and set up the KTO trainer with appropriate training arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubLaM74dM5It"
      },
      "outputs": [],
      "source": [
        "# Model Training Setup\n",
        "\n",
        "# Configure the LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "# Set up the KTO trainer with appropriate training arguments\n",
        "kto_trainer = KTOTrainer(\n",
        "    model=model,\n",
        "    args=KTOConfig(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate=5e-7,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        output_dir=\"outputs\",\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "        seed = 42,\n",
        "        report_to=\"none\", # Use this for WandB etc\n",
        "    ),\n",
        "    train_dataset=train_subset,\n",
        "    #train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IMWJLEdM5Iu"
      },
      "source": [
        "# Training Execution\n",
        "Execute the training process with the configured trainer and monitor the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k76j5NP_M5Iu",
        "outputId": "24ed86e7-4a9b-4d99-c828-71c672afb1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "1.709 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kto_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t_IDtt91ZUli",
        "outputId": "362d55f2-72ba-4965-93cd-6259dd12bf8e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='58' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 58/125 04:52 < 05:49, 0.19 it/s, Epoch 0.46/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.499600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.501600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.499500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.499400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.501200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.499300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.502200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.499300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.501700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.501000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.499300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.498800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.499900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.500800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.499700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.499500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.498700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.500400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.500100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.500600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.499700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.499500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.499400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.498600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.500600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.500300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.497700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.501000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.500600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.498700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.500400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.500100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.496800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.499900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.499300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.499300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.498700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.498600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.500500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.500100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.500100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.499200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.498300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.499300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obai83YGM5Iu"
      },
      "source": [
        "# Model Saving and Export\n",
        "Save the trained model in different formats including LoRA adapters and merged model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaA0FZkKM5Iu"
      },
      "outputs": [],
      "source": [
        "# Model Saving and Export\n",
        "\n",
        "# Local saving\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "\n",
        "# Save merged model as float16 or int4\n",
        "if False: # Set to True to save\n",
        "    model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\")\n",
        "    # model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_4bit\")\n",
        "    # model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"lora\")\n",
        "\n",
        "# Save to HuggingFace Hub\n",
        "if False: # Set to True to save\n",
        "    model.push_to_hub_merged(\"your_name/model\", tokenizer, save_method = \"merged_16bit\", token = \"...\")\n",
        "    # save_method can be \"merged_16bit\", \"merged_4bit\", or \"lora\"\n",
        "\n",
        "# Save to GGUF format (for llama.cpp)\n",
        "if False: # Set to True to save\n",
        "    from transformers import AutoTokenizer\n",
        "    model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\")\n",
        "    !git clone https://github.com/ggerganov/llama.cpp\n",
        "    !cd llama.cpp && make\n",
        "    !python3 llama.cpp/convert.py merged_model/ --outfile model-unsloth.gguf\n",
        "    # Also supports quantization\n",
        "    !./llama.cpp/quantize model-unsloth.gguf model-unsloth-Q4_K_M.gguf Q4_K_M"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ],
      "metadata": {
        "id": "u8QF4Bt9xSJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\",\n",
        "    mapping = {\"role\": \"role\", \"content\": \"content\", \"user\": \"user\", \"assistant\": \"assistant\"},\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def generate_response(message):\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\nQUESTION:\\n\" + \"=\"*50)\n",
        "    print(message + \"\\n\")\n",
        "    print(\"-\"*50 + \"\\nRESPONSE:\\n\" + \"-\"*50)\n",
        "\n",
        "    messages = [{\"content\": message, \"role\": \"user\"}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True,\n",
        "        return_tensors = \"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "    text_streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
        "    outputs = model.generate(\n",
        "        input_ids = inputs,\n",
        "        streamer = text_streamer,\n",
        "        temperature = 0.1,\n",
        "        max_new_tokens = 1024,\n",
        "        use_cache = True\n",
        "    )\n",
        "    return outputs\n",
        "\n",
        "# Test questions\n",
        "questions = [\n",
        "    \"Q:Question: how old julio cesar chavez when he fought de la hoya I found the following answer on Google: He holds records for most successful consecutive defenses of world titles (27), most title fights (37), most title-fight victories (31) and he is after Joe Louis with (23) for most title defenses won by knockout (21). Is that a correct answer? Yes or no.\\nA:\",\n",
        "    \"Q:Information: - The Assistant Secretary of Defense for Health Affairs (ASD(HA)) is chartered under United States Department of Defense Directive (DoDD) 5136.1 in 1994. This DoDD states that the ASD(HA) is the principal advisor to the U.S. Secretary of Defense on all \\\"DoD health policies, programs and activities.\\\" In addition to exercising oversight of all DoD health resources, ASD(HA) serves as director of the Tricare Management Activity. - The Department of the Air Force (DAF) is one of the three Military Departments within the Department of Defense of the United States of America. The Department of the Air Force was formed on September 18, 1947, per the National Security Act of 1947 and it includes all elements and units of the United States Air Force (USAF). - The Surgeon General of the Air Force is the senior-most Medical Service officer in the United States Department of the Air Force. In recent times, this has been a Lieutenant General who serves as head of the United States Air Force Medical Service (AFMS). The Surgeon General is usually the senior Medical Corps officer, but acting surgeons general have been from other branches of the medical service. - Lieutenant general, lieutenant-general and similar (abbrev Lt Gen, LTG and similar) is a three-star military rank (NATO code OF-8) used in many countries. The rank traces its origins to the Middle Ages, where the title of lieutenant general was held by the second in command on the battlefield, who was normally subordinate to a captain general. - The United States Air Force (USAF) is the aerial warfare service branch of the United States Armed Forces and one of the seven American uniformed services. Initially part of the United States Army, the USAF was formed as a separate branch of the military on 18 September 1947 under the National Security Act of 1947. It is the most recent branch of the U.S. military to be formed, and is the largest and one of the world's most technologically advanced air forces. The USAF articulates its core functions as Nuclear Deterrence Operations, Special Operations, Air Superiority, Global Integrated ISR, Space Superiority, Command and Control, Cyberspace Superiority, Personnel Recovery, Global Precision Attack, Building Partnerships, Rapid Global Mobility and Agile Combat Support. - Lieutenant General James Gordon Roudebush , USAF , ( born February 24 , 1948 ) was the 19th Surgeon General of the United States Air Force , Headquarters U.S. Air Force , Washington , D.C. General Roudebush served as functional manager of the U.S. Air Force Medical Service . In this capacity , he advised the Secretary of the Air Force and Air Force Chief of Staff , as well as the Assistant Secretary of Defense for Health Affairs on matters pertaining to the medical aspects of the air expeditionary force and the health of Air Force people . General Roudebush had authority to commit resources worldwide for the Air Force Medical Service , to make decisions affecting the delivery of medical services , and to develop plans , programs and procedures to support worldwide medical service missions . He exercised direction , guidance and technical management of more than 42,400 people assigned to 74 medical facilities worldwide . A native of Gering , Nebraska , Roudebush entered the Air Force in 1975 after receiving a Bachelor of Medicine degree from the University of Nebraska at Lincoln , and a Doctor of Medicine degree from the University of Nebraska College of Medicine . He completed residency training in family practice at the Wright - Patterson Air Force Medical Center , Ohio , in 1978 , and aerospace medicine at Brooks Air Force Base , Texas , in 1984 . He commanded a wing clinic and wing hospital before becoming Deputy Commander of the Air Force Materiel Command Human Systems Center . He has served as Command Surgeon for U.S. Central Command , Pacific Air Forces , U.S. Transportation Command and Headquarters Air Mobility Command . Prior to his selection as the 19th Surgeon General , he served as the Deputy Surgeon General of the U.S. Air Force . He retired from the U.S. Air Force on October 1 , 2009 . After reading the paragraphs above, choose the best answer for the entity that related to 'james g. roudebush' with the relationship of 'occupation'. Choices: - advisor - army - captain - general - lieutenant - military - officer - secretary - surgeon - united states of america\\nA:\",\n",
        "    \"If But slowly and doggedly he went on sawing to and fro., can we conclude that \\\"It was difficult to keep sawing.\\\"?\",\n",
        "    \"You are given a list of queries separated by new line. Your job is to answer with the query that is the most well-formed or well-structured query in terms of grammar, punctuations, or spelling errors.\\nQ: How do you set the alarm on the prospirit watch ?\\nThe allies tried to regain access to the battle of Gallipoli ?\\nWhat is scooter smith real phone number not a fake one ?\\nLaw of Supply and Demand defined ?\\nA:\",\n",
        "    \"How does the sentence end? See options at the end\\n\\nThe woman tried to put the books on the couches but the \\n\\nAvailable options: - couches were too large. - books were too large.\",\n",
        "]\n",
        "# Generate responses\n",
        "for question in questions:\n",
        "    generate_response(question)"
      ],
      "metadata": {
        "id": "iMagn9dzWqgi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}